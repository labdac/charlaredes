{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introducción a redes neuronales\n",
    "\n",
    "![Logo Laboratorio](images/labdac_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Idea de la charla   \n",
    "\n",
    "* Motivar las redes neuronales como modelos predictivos\n",
    "* Presentar las distintas topologías y tipos\n",
    "* Comentar un poco la literatura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### ¿Cómo definimos un modelo?  \n",
    "* Planteamos una hipótesis\n",
    "* Definimos qué es un buen modelo\n",
    "* Intentamos llegar a uno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ¿Cómo definimos un modelo? (Versión matemática)\n",
    "* Planteamos una función predictora\n",
    "* Definimos una función de error (o *costo*)\n",
    "* Minimizamos el costo con métodos numéricos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Funciones predictoras  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Generalmente dos tipos\n",
    " + de regresión\n",
    " + de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* tienen parámetros\n",
    " + algunos se _aprenden_ (calculados según los datos)\n",
    " + otros los decidimos nosotros a priori, con intuición o probando (*hiperparámetros*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Función error/costo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* permite medir cuán bueno (o malo) es el modelo\n",
    "* implícitamente nos dice cómo mejorarlo (¡derivando!)\n",
    "* ejemplo para regresión:  \n",
    "\n",
    "$$ MSE(X,Y) = \\sum_i (Y_i - X_i)^2 $$\n",
    "* ejemplo para clasificación:     \n",
    "\n",
    "$$ \\mathcal{L}(\\theta|x) \\equiv P(x|\\theta) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Optimización: reduciendo el error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Análisis I: derivando, igualo a 0, calculo los puntos de mínimo\n",
    "* Ahora, no siempre es sencillo (o posible) resolverlo de manera analítica, entonces necesitamos un método numérico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Usamos *descenso de gradiente*\n",
    " + dado un campo escalar \n",
    " \n",
    "$$f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$$\n",
    " + y su gradiente:\n",
    "\n",
    "$$\\bar{\\nabla}f(\\bar{x}) = \\left (\n",
    "    \\frac{\\partial f}{\\partial x_1},\n",
    "    ...,\n",
    "    \\frac{\\partial f}{\\partial x_n}\n",
    "\\right )\n",
    "$$\n",
    " + podemos minimizar x iterativamente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\bar{x}^{t+1} =  \\bar{x}^t - \\bar{\\nabla}f(\\bar{x}^t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Comentario: pequeño truquito (SGD)\n",
    "* el gradiente se calcula sobre todos los datos\n",
    "* pero si tenemos millones de datos.. ¡eso es lento!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* entonces lo calculamos sobre unos pocos datos: **Descenso de gradiente estocástico**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualización  \n",
    "![Visualizacion de distintos SGD](images/sgd.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Links y referencias  \n",
    "* Visualizaciones súper copadas y comparación de optimizaciones: [Why momentum really works](https://distill.pub/2017/momentum/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regresión lineal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* super sencillo\n",
    "* hipotesis: \n",
    "$$ y = f(x) = mx + b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![grafico de regresion lineal](images/linear_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* hipótesis generalizada:    \n",
    "\n",
    "$$ h_{\\theta}(\\bar{x}) = \\theta^t \\bar{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* usamos el error cuadrático:  \n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2} \\sum_i \\left ( h_\\theta(x^{(i)}) - y^{(i)} \\right )^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* derivamos:   \n",
    "\n",
    "$$ \\bar{\\nabla} J(\\theta) = \\sum_i \\left ( h_\\theta(x^{(i)}) - y^{(i)}) \\theta \\right )  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* tenemos nuestra receta:     \n",
    "\n",
    "$$\\theta^{t+1} = \\theta^t - \\bar{\\nabla}J(\\theta^t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualización \n",
    "![grafo de regresión lineal](images/grafo_regresion_lineal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo de código\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#contratar un mono que lo programe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regresión logística\n",
    "* queremos transformar nuestra regresión lineal para que prediga una variable binaria"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
