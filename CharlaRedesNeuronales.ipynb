{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introducción a redes neuronales\n",
    "\n",
    "![Logo Laboratorio](images/labdac_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Idea de la charla   \n",
    "\n",
    "* Motivar las redes neuronales como modelos predictivos\n",
    "* Presentar las distintas topologías y tipos\n",
    "* Comentar un poco la literatura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### ¿Cómo definimos un modelo?  \n",
    "* Planteamos una hipótesis\n",
    "* Definimos qué es un buen modelo\n",
    "* Intentamos llegar a uno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ¿Cómo definimos un modelo? (Versión matemática)\n",
    "* Planteamos una función predictora\n",
    "* Definimos una función de error (o *costo*)\n",
    "* Minimizamos el costo con métodos numéricos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Funciones predictoras  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Generalmente dos tipos\n",
    " + de regresión\n",
    " + de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* tienen parámetros\n",
    " + algunos se _aprenden_ (calculados según los datos)\n",
    " + otros los decidimos nosotros a priori, con intuición o probando (*hiperparámetros*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Función error/costo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* permite medir cuán bueno (o malo) es el modelo\n",
    "* implícitamente nos dice cómo mejorarlo (¡derivando!)\n",
    "* ejemplo para regresión:  \n",
    "\n",
    "$$ MSE(X,Y) = \\sum_i (Y_i - X_i)^2 $$\n",
    "* ejemplo para clasificación:     \n",
    "\n",
    "$$ \\mathcal{L}(\\theta|x) \\equiv P(x|\\theta) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Optimización: reduciendo el error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Análisis I: derivando, igualo a 0, calculo los puntos de mínimo\n",
    "* Ahora, no siempre es sencillo (o posible) resolverlo de manera analítica, entonces necesitamos un método numérico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Usamos *descenso de gradiente*\n",
    " + dado un campo escalar \n",
    " \n",
    "$$f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$$\n",
    " + y su gradiente:\n",
    "\n",
    "$$\\bar{\\nabla}f(\\bar{x}) = \\left (\n",
    "    \\frac{\\partial f}{\\partial x_1},\n",
    "    ...,\n",
    "    \\frac{\\partial f}{\\partial x_n}\n",
    "\\right )\n",
    "$$\n",
    " + podemos minimizar x iterativamente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\bar{x}^{t+1} =  \\bar{x}^t - \\bar{\\nabla}f(\\bar{x}^t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Comentario: pequeño truquito (SGD)\n",
    "* el gradiente se calcula sobre todos los datos\n",
    "* pero si tenemos millones de datos.. ¡eso es lento!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* entonces lo calculamos sobre unos pocos datos: **Descenso de gradiente estocástico**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualización  \n",
    "![Visualizacion de distintos SGD](images/sgd.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Links y referencias  \n",
    "* Visualizaciones súper copadas y comparación de optimizaciones: [Why momentum really works](https://distill.pub/2017/momentum/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regresión lineal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* super sencillo\n",
    "* hipotesis: \n",
    "$$ y = f(x) = mx + b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![grafico de regresion lineal](images/linear_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* hipótesis generalizada:    \n",
    "\n",
    "$$ h_{\\theta}(\\bar{x}) = \\theta^t \\bar{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* usamos el error cuadrático:  \n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2} \\sum_i \\left ( h_\\theta(x^{(i)}) - y^{(i)} \\right )^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* derivamos:   \n",
    "\n",
    "$$ \\bar{\\nabla} J(\\theta) = \\sum_i \\left ( h_\\theta(x^{(i)}) - y^{(i)}) \\theta \\right )  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* tenemos nuestra receta:     \n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\bar{\\nabla}J(\\theta_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualización \n",
    "![grafo de regresión lineal](images/grafo_regresion_lineal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo de código\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#contratar un mono que lo programe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regresión logística\n",
    "* queremos transformar nuestra regresión lineal para que prediga una variable binaria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* teníamos una función \n",
    "$$f_{regr}: \\mathbb{R}^n \\rightarrow \\mathbb{R}$$\n",
    "* ahora queremos \n",
    "$$f_{clasif}: \\mathbb{R}^n \\rightarrow [0,1]$$  \n",
    "* esto lo podemos conseguir aplicando una función $\\sigma: \\mathbb{R} \\rightarrow [0,1]$\n",
    "* particularmente estaría bueno que\n",
    " + $\\sigma(x)$ sea fácil de diferenciar\n",
    " + $\\sigma(x)$ sea continua y tenga propiedades lindas\n",
    " + $\\sigma(x)$ sea no lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* *un* candidato es la función logística:   \n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Esta función es buena porque nos permite indicar el grado de incerteza que podemos tener para predecir la clase. \n",
    "![curva sigmoide](images/sigmoid_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Receta de la logística\n",
    "* hipótesis:    \n",
    "\n",
    "$$ P(y=1|x) = h_\\theta(x) = \\frac{1}{1+exp(-\\theta^tx)}$$   \n",
    "\n",
    "$$ P(y=0|x) = 1 - P(y=1|x) = 1 - h_\\theta(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* para definir el costo, usamos la verosimilitud:   \n",
    "\n",
    "$$ \\mathcal{L}(\\theta|x_1\\ ...\\ x_n) = P(x_1\\ ...\\ x_n|\\theta) = P(x_1|\\theta)\\times ... \\times P(x_n|\\theta)$$  \n",
    "\n",
    "entonces:  \n",
    "\n",
    "$$ J(\\theta) = -\\sum_i \\log \\mathcal{L(\\theta|x_i)}$$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ J(\\theta) = -\\sum_i \\left ( y_i \\log h_\\theta(x_i) + (1-y_i) \\log(1-h_\\theta(x_i)) \\right )$$\n",
    "  \n",
    "* luego de otra hoja, obtenemos el gradiente:   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\bar{\\nabla}J(\\theta) = \\sum_i x_i(h_\\theta(x_i)-y_i)$$   \n",
    "\n",
    "¡bastante sencillo quedó el gradiente!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Representación de la regresión logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![grafo de la regresión logística](images/graph_logistic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ¡Veamos un ejemplo!\n",
    "** [playground.tensorflow.org](http://playground.tensorflow.org/#activation=sigmoid&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=&seed=0.90322&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)**"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
